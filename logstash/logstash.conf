input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["dashboard-messages"]
    codec => json
    group_id => "logstash-dashboard"
    auto_offset_reset => "latest"
    consumer_threads => 1
    decorate_events => true
  }
}

filter {
  # Add unique ID to prevent duplicates
  fingerprint {
    source => ["message", "sender", "timestamp", "value"]
    target => "[@metadata][fingerprint]"
    method => "SHA256"
  }
  
  # Parse timestamp
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
    remove_field => ["timestamp"]
  }
  
  # Add metadata
  mutate {
    add_field => {
      "source_system" => "kafka-dashboard"
      "processed_at" => "%{@timestamp}"
    }
    
    # Ensure types
    convert => {
      "value" => "integer"
    }
    
    # Add unique identifier
    add_field => {
      "message_id" => "%{[@metadata][fingerprint]}"
    }
  }
}


output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "kafka-dashboard-live"  # Separate index for live messages
    document_id => "%{[@metadata][fingerprint]}"  # Use fingerprint as ID to prevent duplicates
    action => "create"  # Only create new documents, don't update existing ones
  }
  
  # For debugging
  stdout {
    codec => rubydebug {
      metadata => true
    }
  }
}